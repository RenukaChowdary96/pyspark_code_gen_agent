{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b36f9fe",
   "metadata": {},
   "source": [
    "# ETL Pipeline - Generated Code\n",
    "        \n",
    "**Generated:** 2025-09-04 19:01:58  \n",
    "**Configuration:** Text: Text file (7656 chars)\n",
    "\n",
    "## Overview\n",
    "This notebook contains the auto-generated ETL pipeline code for migrating data from Oracle to Databricks Delta Lake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, isnull\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Create a SparkSession with Delta Lake extensions.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.appName(\"Sales ETL\") \\\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.1.0\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "def load_config(spark):\n",
    "    \"\"\"\n",
    "    Load configuration from environment variables.\n",
    "    \"\"\"\n",
    "    db_username = os.environ.get(\"DB_USERNAME\")\n",
    "    db_password = os.environ.get(\"DB_PASSWORD\")\n",
    "    db_host = os.environ.get(\"DB_HOST\")\n",
    "    db_port = os.environ.get(\"DB_PORT\")\n",
    "    db_name = os.environ.get(\"DB_NAME\")\n",
    "    delta_lake_path = os.environ.get(\"DELTA_LAKE_PATH\")\n",
    "\n",
    "    # Create a dictionary to store the configuration\n",
    "    config = {\n",
    "        \"db_username\": db_username,\n",
    "        \"db_password\": db_password,\n",
    "        \"db_host\": db_host,\n",
    "        \"db_port\": db_port,\n",
    "        \"db_name\": db_name,\n",
    "        \"delta_lake_path\": delta_lake_path\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def load_data(spark, config):\n",
    "    \"\"\"\n",
    "    Load data from the database using the provided configuration.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load customers data\n",
    "        customers_df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:postgresql://{config['db_host']}:{config['db_port']}/{config['db_name']}\") \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .option(\"dbtable\", \"SALES_DB.CUSTOMERS\") \\\n",
    "            .option(\"user\", config[\"db_username\"]) \\\n",
    "            .option(\"password\", config[\"db_password\"]) \\\n",
    "            .load()\n",
    "\n",
    "        # Load products data\n",
    "        products_df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:postgresql://{config['db_host']}:{config['db_port']}/{config['db_name']}\") \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .option(\"dbtable\", \"SALES_DB.PRODUCTS\") \\\n",
    "            .option(\"user\", config[\"db_username\"]) \\\n",
    "            .option(\"password\", config[\"db_password\"]) \\\n",
    "            .load()\n",
    "\n",
    "        # Load sales data\n",
    "        sales_df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:postgresql://{config['db_host']}:{config['db_port']}/{config['db_name']}\") \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .option(\"dbtable\", \"SALES_DB.SALES\") \\\n",
    "            .option(\"user\", config[\"db_username\"]) \\\n",
    "            .option(\"password\", config[\"db_password\"]) \\\n",
    "            .load()\n",
    "\n",
    "        # Filter customers by status\n",
    "        filtered_customers_df = customers_df.filter(col(\"STATUS\") == \"ACTIVE\")\n",
    "\n",
    "        # Filter sales by quantity and total amount\n",
    "        filtered_sales_df = sales_df.filter((col(\"QUANTITY\") > 0) & (col(\"TOTAL_AMOUNT\") > 0))\n",
    "\n",
    "        # Perform data quality checks\n",
    "        logger.info(\"Performing data quality checks...\")\n",
    "        customers_null_count = filtered_customers_df.select(count(isnull(\"CUSTOMER_ID\")).alias(\"null_count\")).collect()[0].null_count\n",
    "        products_null_count = products_df.select(count(isnull(\"PRODUCT_ID\")).alias(\"null_count\")).collect()[0].null_count\n",
    "        sales_null_count = filtered_sales_df.select(count(isnull(\"SALE_ID\")).alias(\"null_count\")).collect()[0].null_count\n",
    "\n",
    "        logger.info(f\"Customers null count: {customers_null_count}\")\n",
    "        logger.info(f\"Products null count: {products_null_count}\")\n",
    "        logger.info(f\"Sales null count: {sales_null_count}\")\n",
    "\n",
    "        return filtered_customers_df, products_df, filtered_sales_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def transform_data(customers_df, products_df, sales_df):\n",
    "    \"\"\"\n",
    "    Transform the data by joining the sales data with the customers and products data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Join sales data with customers data\n",
    "        sales_customers_df = sales_df.join(broadcast(customers_df), \"CUSTOMER_ID\", \"inner\")\n",
    "\n",
    "        # Join sales_customers data with products data\n",
    "        sales_customers_products_df = sales_customers_df.join(broadcast(products_df), \"PRODUCT_ID\", \"inner\")\n",
    "\n",
    "        # Perform monthly aggregation by customer and product\n",
    "        aggregated_df = sales_customers_products_df.groupBy(\"CUSTOMER_ID\", \"PRODUCT_ID\", \"SALE_DATE\").agg({\"TOTAL_AMOUNT\": \"sum\"})\n",
    "\n",
    "        return aggregated_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error transforming data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_data_to_delta_lake(df, config):\n",
    "    \"\"\"\n",
    "    Load the transformed data to Delta Lake.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Write the data to Delta Lake\n",
    "        df.write.format(\"delta\") \\\n",
    "            .option(\"path\", config[\"delta_lake_path\"]) \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .partitionBy(\"SALE_DATE\") \\\n",
    "            .save()\n",
    "\n",
    "        logger.info(\"Data loaded to Delta Lake successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data to Delta Lake: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Create a SparkSession\n",
    "    spark = create_spark_session()\n",
    "\n",
    "    # Load configuration\n",
    "    config = load_config(spark)\n",
    "\n",
    "    # Load data\n",
    "    customers_df, products_df, sales_df = load_data(spark, config)\n",
    "\n",
    "    # Transform data\n",
    "    aggregated_df = transform_data(customers_df, products_df, sales_df)\n",
    "\n",
    "    # Load data to Delta Lake\n",
    "    load_data_to_delta_lake(aggregated_df, config)\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c85bd",
   "metadata": {},
   "source": [
    "## Validation Report\n",
    "\n",
    "**Summary:** 9/9 checks passed\n",
    "\n",
    "| Check | Status | Details |\n",
    "|-------|--------|---------|\n",
    "| SparkSession | PASS PASS | SparkSession properly initialized |\n",
    "| Delta Lake | PASS PASS | Delta Lake format detected |\n",
    "| Environment Variables | PASS PASS | Uses environment variables |\n",
    "| No Hardcoded Creds | PASS PASS | No hardcoded credentials found |\n",
    "| Predicate Pushdown | PASS PASS | Database-level filtering detected |\n",
    "| Broadcast Joins | PASS PASS | Broadcast joins implemented |\n",
    "| Error Handling | PASS PASS | Exception handling present |\n",
    "| Logging | PASS PASS | Logging implemented |\n",
    "| Data Quality Checks | PASS PASS | Data quality checks present |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673ba8d",
   "metadata": {},
   "source": [
    "## Test Report\n",
    "\n",
    "**Summary:** 5/6 tests passed\n",
    "\n",
    "| Test | Status | Input | Expected | Output |\n",
    "|------|--------|-------|----------|--------|\n",
    "| Syntax Validation | PASS | Python code compilation | Valid Python syntax | Code compiles successfully |\n",
    "| Business Rules Filter | PASS | 3 records with mixed status/values | 1 valid record | 1 records after filtering |\n",
    "| Data Transformation | PASS | Sales with dates | Year/month extraction | 2 unique year-month combinations |\n",
    "| Aggregation Logic | PASS | 4 records to aggregate | Customer 1, Product 10: qty=8, amt=80 | Aggregation produces 3 groups |\n",
    "| Data Volume Handling | PASS | Simulated 1,000,000 records | Handles large volumes | Volume test passed |\n",
    "| Performance Optimizations | FAIL | Code analysis | Performance features | Found: broadcast joins |\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

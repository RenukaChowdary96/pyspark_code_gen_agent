{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c1ecb1",
   "metadata": {},
   "source": [
    "# ETL Pipeline - Generated Code\n",
    "        \n",
    "**Generated:** 2025-09-04 17:48:15  \n",
    "**Configuration:** Text: Text file (6208 chars)\n",
    "\n",
    "## Overview\n",
    "This notebook contains the auto-generated ETL pipeline code for migrating data from Oracle to Databricks Delta Lake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e549ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, isnull\n",
    "\n",
    "# Initialize logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "SALES_DB_HOST = os.environ.get('SALES_DB_HOST')\n",
    "SALES_DB_PORT = os.environ.get('SALES_DB_PORT')\n",
    "SALES_DB_USER = os.environ.get('SALES_DB_USER')\n",
    "SALES_DB_PASSWORD = os.environ.get('SALES_DB_PASSWORD')\n",
    "SALES_DB_NAME = os.environ.get('SALES_DB_NAME')\n",
    "\n",
    "# Create SparkSession with Delta Lake extensions\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sales ETL Pipeline\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.1.1\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set up database connection properties\n",
    "db_properties = {\n",
    "    \"host\": SALES_DB_HOST,\n",
    "    \"port\": SALES_DB_PORT,\n",
    "    \"user\": SALES_DB_USER,\n",
    "    \"password\": SALES_DB_PASSWORD,\n",
    "    \"database\": SALES_DB_NAME\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58f655",
   "metadata": {},
   "source": [
    "## Validation Report\n",
    "\n",
    "**Summary:** 5/9 checks passed\n",
    "\n",
    "| Check | Status | Details |\n",
    "|-------|--------|---------|\n",
    "| SparkSession | PASS PASS | SparkSession properly initialized |\n",
    "| Delta Lake | PASS PASS | Delta Lake format detected |\n",
    "| Environment Variables | PASS PASS | Uses environment variables |\n",
    "| No Hardcoded Creds | PASS PASS | No hardcoded credentials found |\n",
    "| Predicate Pushdown | FAIL FAIL (Performance) | No predicate pushdown optimization |\n",
    "| Broadcast Joins | FAIL FAIL (Performance) | No broadcast join optimization |\n",
    "| Error Handling | FAIL FAIL (Important) | Missing try/except blocks |\n",
    "| Logging | PASS PASS | Logging implemented |\n",
    "| Data Quality Checks | FAIL FAIL (Best Practice) | No data quality checks |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c0778",
   "metadata": {},
   "source": [
    "## Test Report\n",
    "\n",
    "**Summary:** 5/6 tests passed\n",
    "\n",
    "| Test | Status | Input | Expected | Output |\n",
    "|------|--------|-------|----------|--------|\n",
    "| Syntax Validation | PASS | Python code compilation | Valid Python syntax | Code compiles successfully |\n",
    "| Business Rules Filter | PASS | 3 records with mixed status/values | 1 valid record | 1 records after filtering |\n",
    "| Data Transformation | PASS | Sales with dates | Year/month extraction | 2 unique year-month combinations |\n",
    "| Aggregation Logic | PASS | 4 records to aggregate | Customer 1, Product 10: qty=8, amt=80 | Aggregation produces 3 groups |\n",
    "| Data Volume Handling | PASS | Simulated 1,000,000 records | Handles large volumes | Volume test passed |\n",
    "| Performance Optimizations | FAIL | Code analysis | Performance features | Found: none |\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

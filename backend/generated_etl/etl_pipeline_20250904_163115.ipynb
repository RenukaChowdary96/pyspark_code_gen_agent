{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fe80bcb",
   "metadata": {},
   "source": [
    "# ETL Pipeline - Generated Code\n",
    "        \n",
    "**Generated:** 2025-09-04 16:31:15  \n",
    "**Configuration:** YAML: YAML config with keys: metadata, source, target...\n",
    "\n",
    "## Overview\n",
    "This notebook contains the auto-generated ETL pipeline code for migrating data from Oracle to Databricks Delta Lake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df140e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, month, year\n",
    "import logging\n",
    "\n",
    "# Initialize logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_config(file_path):\n",
    "    \"\"\"Load YAML configuration file.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"Create SparkSession with Delta Lake extensions.\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Sales ETL\") \\\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.0.0\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "def load_data(spark, config):\n",
    "    \"\"\"Load data from source database.\"\"\"\n",
    "    source_url = config['source']['url']\n",
    "    username = os.environ['SALES_DB_USERNAME']\n",
    "    password = os.environ['SALES_DB_PASSWORD']\n",
    "    database = config['metadata']['database']\n",
    "    tables = config['metadata']['tables']\n",
    "\n",
    "    # Load CUSTOMERS table\n",
    "    customers_df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", source_url) \\\n",
    "        .option(\"username\", username) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .option(\"dbtable\", f\"{database}.CUSTOMERS\") \\\n",
    "        .option(\"predicate\", \"STATUS = 'ACTIVE'\") \\\n",
    "        .load()\n",
    "\n",
    "    # Load PRODUCTS table\n",
    "    products_df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", source_url) \\\n",
    "        .option(\"username\", username) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .option(\"dbtable\", f\"{database}.PRODUCTS\") \\\n",
    "        .load()\n",
    "\n",
    "    # Load SALES table\n",
    "    sales_df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", source_url) \\\n",
    "        .option(\"username\", username) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .option(\"dbtable\", f\"{database}.SALES\") \\\n",
    "        .option(\"predicate\", \"QUANTITY > 0 AND TOTAL_AMOUNT > 0\") \\\n",
    "        .load()\n",
    "\n",
    "    return customers_df, products_df, sales_df\n",
    "\n",
    "def transform_data(customers_df, products_df, sales_df):\n",
    "    \"\"\"Transform data by joining and aggregating.\"\"\"\n",
    "    # Broadcast joins for dimension tables\n",
    "    sales_df = sales_df.join(broadcast(customers_df), \"CUSTOMER_ID\") \\\n",
    "        .join(broadcast(products_df), \"PRODUCT_ID\")\n",
    "\n",
    "    # Monthly aggregation by customer and product\n",
    "    aggregated_df = sales_df.groupBy(col(\"CUSTOMER_ID\"), col(\"CUSTOMER_NAME\"), col(\"PRODUCT_ID\"), col(\"PRODUCT_NAME\"), month(\"SALE_DATE\").alias(\"MONTH\"), year(\"SALE_DATE\").alias(\"YEAR\")) \\\n",
    "        .agg({\"TOTAL_AMOUNT\": \"sum\", \"QUANTITY\": \"sum\"})\n",
    "\n",
    "    return aggregated_df\n",
    "\n",
    "def load_data_to_delta(spark, aggregated_df, config):\n",
    "    \"\"\"Load data to Delta Lake.\"\"\"\n",
    "    target_url = config['target']['url']\n",
    "    format = config['target']['format']\n",
    "\n",
    "    # Write data to Delta Lake with partitioning\n",
    "    aggregated_df.write.format(format) \\\n",
    "        .partitionBy(\"YEAR\", \"MONTH\") \\\n",
    "        .save(target_url)\n",
    "\n",
    "def data_quality_checks(aggregated_df):\n",
    "    \"\"\"Perform data quality checks.\"\"\"\n",
    "    # Check for null values\n",
    "    null_counts = aggregated_df.select([count(when(isnull(c), c)).alias(c) for c in aggregated_df.columns])\n",
    "    logger.info(\"Null counts:\")\n",
    "    null_counts.show()\n",
    "\n",
    "    # Check for duplicate values\n",
    "    duplicate_counts = aggregated_df.groupBy(\"CUSTOMER_ID\", \"CUSTOMER_NAME\", \"PRODUCT_ID\", \"PRODUCT_NAME\", \"MONTH\", \"YEAR\").count()\n",
    "    logger.info(\"Duplicate counts:\")\n",
    "    duplicate_counts.show()\n",
    "\n",
    "def main():\n",
    "    # Load configuration\n",
    "    config_file = \"config.yaml\"\n",
    "    config = load_config(config_file)\n",
    "\n",
    "    # Create SparkSession\n",
    "    spark = create_spark_session()\n",
    "\n",
    "    try:\n",
    "        # Load data from source database\n",
    "        customers_df, products_df, sales_df = load_data(spark, config)\n",
    "        logger.info(\"Data loaded from source database.\")\n",
    "\n",
    "        # Transform data\n",
    "        aggregated_df = transform_data(customers_df, products_df, sales_df)\n",
    "        logger.info(\"Data transformed.\")\n",
    "\n",
    "        # Perform data quality checks\n",
    "        data_quality_checks(aggregated_df)\n",
    "        logger.info(\"Data quality checks completed.\")\n",
    "\n",
    "        # Load data to Delta Lake\n",
    "        load_data_to_delta(spark, aggregated_df, config)\n",
    "        logger.info(\"Data loaded to Delta Lake.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Stop SparkSession\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565bac56",
   "metadata": {},
   "source": [
    "## Validation Report\n",
    "\n",
    "**Summary:** 8/9 checks passed\n",
    "\n",
    "| Check | Status | Details |\n",
    "|-------|--------|---------|\n",
    "| SparkSession | PASS PASS | SparkSession properly initialized |\n",
    "| Delta Lake | PASS PASS | Delta Lake format detected |\n",
    "| Environment Variables | PASS PASS | Uses environment variables |\n",
    "| No Hardcoded Creds | PASS PASS | No hardcoded credentials found |\n",
    "| Predicate Pushdown | FAIL FAIL (Performance) | No predicate pushdown optimization |\n",
    "| Broadcast Joins | PASS PASS | Broadcast joins implemented |\n",
    "| Error Handling | PASS PASS | Exception handling present |\n",
    "| Logging | PASS PASS | Logging implemented |\n",
    "| Data Quality Checks | PASS PASS | Data quality checks present |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c3c248",
   "metadata": {},
   "source": [
    "## Test Report\n",
    "\n",
    "**Summary:** 5/6 tests passed\n",
    "\n",
    "| Test | Status | Input | Expected | Output |\n",
    "|------|--------|-------|----------|--------|\n",
    "| Syntax Validation | PASS | Python code compilation | Valid Python syntax | Code compiles successfully |\n",
    "| Business Rules Filter | PASS | 3 records with mixed status/values | 1 valid record | 1 records after filtering |\n",
    "| Data Transformation | PASS | Sales with dates | Year/month extraction | 2 unique year-month combinations |\n",
    "| Aggregation Logic | PASS | 4 records to aggregate | Customer 1, Product 10: qty=8, amt=80 | Aggregation produces 3 groups |\n",
    "| Data Volume Handling | PASS | Simulated 1,000,000 records | Handles large volumes | Volume test passed |\n",
    "| Performance Optimizations | FAIL | Code analysis | Performance features | Found: broadcast joins |\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

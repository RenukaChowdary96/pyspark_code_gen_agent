{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8275b5",
   "metadata": {},
   "source": [
    "# ETL Pipeline - Generated Code\n",
    "        \n",
    "**Generated:** 2025-09-04 16:51:42  \n",
    "**Configuration:** YAML: YAML config with keys: metadata, source, target...\n",
    "\n",
    "## Overview\n",
    "This notebook contains the auto-generated ETL pipeline code for migrating data from Oracle to Databricks Delta Lake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5498fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oracle to Databricks ETL Pipeline - Generated Code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"ETL_Pipeline\")\n",
    "\n",
    "# Load config from environment\n",
    "ORACLE_HOST = os.environ.get('ORACLE_HOST', 'localhost')\n",
    "ORACLE_PORT = os.environ.get('ORACLE_PORT', '1521')\n",
    "ORACLE_SERVICE = os.environ.get('ORACLE_SERVICE', 'XE')\n",
    "ORACLE_USERNAME = os.environ.get('ORACLE_USERNAME')\n",
    "ORACLE_PASSWORD = os.environ.get('ORACLE_PASSWORD')\n",
    "\n",
    "# Validate credentials\n",
    "if not ORACLE_USERNAME or not ORACLE_PASSWORD:\n",
    "    raise ValueError(\"Oracle credentials not found in environment variables\")\n",
    "\n",
    "ORACLE_URL = f\"jdbc:oracle:thin:@{ORACLE_HOST}:{ORACLE_PORT}:{ORACLE_SERVICE}\"\n",
    "DELTA_LAKE_LOCATION = os.environ.get('DELTA_LAKE_LOCATION', '/tmp/delta-lake')\n",
    "\n",
    "# Create SparkSession with optimizations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Customer Product Monthly Sales\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logger.info(\"Spark session created with Delta Lake support\")\n",
    "\n",
    "# Oracle connection properties\n",
    "oracle_props = {\n",
    "    \"user\": ORACLE_USERNAME,\n",
    "    \"password\": ORACLE_PASSWORD,\n",
    "    \"driver\": \"oracle.jdbc.driver.OracleDriver\",\n",
    "    \"fetchsize\": \"10000\"\n",
    "}\n",
    "\n",
    "\n",
    "# ==================== MAIN ETL LOGIC ====================\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, isnull\n",
    "\n",
    "# Load YAML configuration\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Create SparkSession with Delta Lake extensions\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sales ETL Pipeline\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.1.1\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set environment variables for credentials\n",
    "db_username = os.environ['DB_USERNAME']\n",
    "db_password = os.environ['DB_PASSWORD']\n",
    "db_host = os.environ['DB_HOST']\n",
    "db_port = os.environ['DB_PORT']\n",
    "db_database = config['metadata']['database']\n",
    "\n",
    "# Create logger\n",
    "logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "\n",
    "# Load data from database\n",
    "try:\n",
    "    logger.info(\"Loading data from database...\")\n",
    "    customers_df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", f\"jdbc:postgresql://{db_host}:{db_port}/{db_database}\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .option(\"dbtable\", \"CUSTOMERS\") \\\n",
    "        .option(\"user\", db_username) \\\n",
    "        .option(\"password\", db_password) \\\n",
    "        .load() \\\n",
    "        .filter(col(\"STATUS\") == \"ACTIVE\")\n",
    "\n",
    "    products_df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", f\"jdbc:postgresql://{db_host}:{db_port}/{db_database}\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .option(\"dbtable\", \"PRODUCTS\") \\\n",
    "        .option(\"user\", db_username) \\\n",
    "        .option(\"password\", db_password) \\\n",
    "        .load()\n",
    "\n",
    "    sales_df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", f\"jdbc:postgresql://{db_host}:{db_port}/{db_database}\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .option(\"dbtable\", \"SALES\") \\\n",
    "        .option(\"user\", db_username) \\\n",
    "        .option(\"password\", db_password) \\\n",
    "        .load() \\\n",
    "        .filter((col(\"QUANTITY\") > 0) & (col(\"TOTAL_AMOUNT\") > 0))\n",
    "\n",
    "    logger.info(\"Data loaded successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading data: {str(e)}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# Perform data quality checks\n",
    "try:\n",
    "    logger.info(\"Performing data quality checks...\")\n",
    "    customers_null_count = customers_df.select(count(isnull(\"CUSTOMER_ID\")).alias(\"null_count\")).collect()[0].null_count\n",
    "    products_null_count = products_df.select(count(isnull(\"PRODUCT_ID\")).alias(\"null_count\")).collect()[0].null_count\n",
    "    sales_null_count = sales_df.select(count(isnull(\"SALE_ID\")).alias(\"null_count\")).collect()[0].null_count\n",
    "\n",
    "    logger.info(f\"Customers null count: {customers_null_count}\")\n",
    "    logger.info(f\"Products null count: {products_null_count}\")\n",
    "    logger.info(f\"Sales null count: {sales_null_count}\")\n",
    "\n",
    "    if customers_null_count > 0 or products_null_count > 0 or sales_null_count > 0:\n",
    "        logger.error(\"Data quality checks failed. Null values found.\")\n",
    "        spark.stop()\n",
    "        exit(1)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error performing data quality checks: {str(e)}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# Join sales with customers and products using broadcast\n",
    "try:\n",
    "    logger.info(\"Joining sales with customers and products...\")\n",
    "    sales_joined_df = sales_df.join(broadcast(customers_df), \"CUSTOMER_ID\", \"inner\") \\\n",
    "        .join(broadcast(products_df), \"PRODUCT_ID\", \"inner\")\n",
    "\n",
    "    logger.info(\"Join successful.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error joining sales with customers and products: {str(e)}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# Perform monthly aggregation\n",
    "try:\n",
    "    logger.info(\"Performing monthly aggregation...\")\n",
    "    aggregated_df = sales_joined_df.groupBy(col(\"CUSTOMER_ID\"), col(\"PRODUCT_ID\"), col(\"SALE_DATE\").substr(1, 7).alias(\"MONTH\")) \\\n",
    "        .agg(count(\"SALE_ID\").alias(\"SALES_COUNT\"), sum(\"TOTAL_AMOUNT\").alias(\"TOTAL_SALES\"))\n",
    "\n",
    "    logger.info(\"Aggregation successful.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error performing monthly aggregation: {str(e)}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# Write output to Delta Lake format with partitioning\n",
    "try:\n",
    "    logger.info(\"Writing output to Delta Lake format...\")\n",
    "    aggregated_df.write.format(\"delta\") \\\n",
    "        .option(\"path\", config['target']['path']) \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .partitionBy(\"MONTH\") \\\n",
    "        .save()\n",
    "\n",
    "    logger.info(\"Output written successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error writing output to Delta Lake format: {str(e)}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e30de3",
   "metadata": {},
   "source": [
    "## Validation Report\n",
    "\n",
    "**Summary:** 9/9 checks passed\n",
    "\n",
    "| Check | Status | Details |\n",
    "|-------|--------|---------|\n",
    "| SparkSession | PASS PASS | SparkSession properly initialized |\n",
    "| Delta Lake | PASS PASS | Delta Lake format detected |\n",
    "| Environment Variables | PASS PASS | Uses environment variables |\n",
    "| No Hardcoded Creds | PASS PASS | No hardcoded credentials found |\n",
    "| Predicate Pushdown | PASS PASS | Database-level filtering detected |\n",
    "| Broadcast Joins | PASS PASS | Broadcast joins implemented |\n",
    "| Error Handling | PASS PASS | Exception handling present |\n",
    "| Logging | PASS PASS | Logging implemented |\n",
    "| Data Quality Checks | PASS PASS | Data quality checks present |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41194dab",
   "metadata": {},
   "source": [
    "## Test Report\n",
    "\n",
    "**Summary:** 6/6 tests passed\n",
    "\n",
    "| Test | Status | Input | Expected | Output |\n",
    "|------|--------|-------|----------|--------|\n",
    "| Syntax Validation | PASS | Python code compilation | Valid Python syntax | Code compiles successfully |\n",
    "| Business Rules Filter | PASS | 3 records with mixed status/values | 1 valid record | 1 records after filtering |\n",
    "| Data Transformation | PASS | Sales with dates | Year/month extraction | 2 unique year-month combinations |\n",
    "| Aggregation Logic | PASS | 4 records to aggregate | Customer 1, Product 10: qty=8, amt=80 | Aggregation produces 3 groups |\n",
    "| Data Volume Handling | PASS | Simulated 1,000,000 records | Handles large volumes | Volume test passed |\n",
    "| Performance Optimizations | PASS | Code analysis | Performance features | Found: broadcast joins, partitioning, adaptive query |\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
